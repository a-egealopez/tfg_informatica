{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "375b97e1",
      "metadata": {
        "id": "375b97e1"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) MONAI Consortium\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d71306f",
      "metadata": {
        "id": "9d71306f"
      },
      "source": [
        "# Denoising Diffusion Probabilistic Models with MedNIST Dataset\n",
        "\n",
        "This tutorial illustrates how to use MONAI for training a denoising diffusion probabilistic model (DDPM)[1] to create\n",
        "synthetic 2D images.\n",
        "\n",
        "[1] - Ho et al. \"Denoising Diffusion Probabilistic Models\" https://arxiv.org/abs/2006.11239\n",
        "\n",
        "\n",
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aa3774e",
      "metadata": {
        "id": "6aa3774e",
        "outputId": "abbc8261-178d-482b-929b-647a6acfd1ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
            "2025-12-17 20:02:30.546665: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766001750.568781   18644 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766001750.575453   18644 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766001750.592783   18644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766001750.592820   18644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766001750.592826   18644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766001750.592830   18644 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-17 20:02:30.598298: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "!python -c \"import monai\" || pip install -q \"monai-weekly[tqdm]\"\n",
        "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
        "############################################################\n",
        "!pip install -q medmnist\n",
        "############################################################\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3154fee",
      "metadata": {
        "id": "f3154fee"
      },
      "source": [
        "## Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd62a552",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "dd62a552"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from monai import transforms\n",
        "from monai.config import print_config\n",
        "from monai.data import Dataset, CacheDataset, DataLoader\n",
        "from monai.utils import first, set_determinism\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "\n",
        "############################################################\n",
        "#from monai.apps import MedNISTDataset\n",
        "from medmnist import INFO\n",
        "from medmnist import PathMNIST\n",
        "from torchvision import transforms as tv_transforms\n",
        "############################################################\n",
        "\n",
        "############################################################\n",
        "#from generative.inferers import DiffusionInferer\n",
        "#from generative.networks.nets import DiffusionModelUNet\n",
        "#from generative.networks.schedulers import DDPMScheduler\n",
        "from monai.inferers import DiffusionInferer\n",
        "from monai.networks.nets import DiffusionModelUNet\n",
        "from monai.networks.schedulers import DDPMScheduler\n",
        "############################################################\n",
        "\n",
        "############################################################\n",
        "import cv2\n",
        "############################################################\n",
        "\n",
        "print_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizamos las clases de PathMNIST"
      ],
      "metadata": {
        "id": "32HyVvu3sElF"
      },
      "id": "32HyVvu3sElF"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_info = INFO['pathmnist']\n",
        "label_dict = dataset_info['label']\n",
        "\n",
        "preview_data = PathMNIST(split=\"train\", download=True)\n",
        "\n",
        "fig, axes = plt.subplots(1, 9, figsize=(20, 5))\n",
        "found_classes = {}\n",
        "\n",
        "for img, label in zip(preview_data.imgs, preview_data.labels):\n",
        "    idx = label[0]\n",
        "    if idx not in found_classes:\n",
        "        found_classes[idx] = img\n",
        "    if len(found_classes) == 9: break\n",
        "\n",
        "for i in range(9):\n",
        "    axes[i].imshow(found_classes[i])\n",
        "    axes[i].set_title(f\"{i}: {label_dict[str(i)]}\", fontsize=10)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FOkCQYP1sCdD"
      },
      "id": "FOkCQYP1sCdD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "be99fa93",
      "metadata": {
        "id": "be99fa93"
      },
      "source": [
        "## Setup data directory\n",
        "\n",
        "You can specify a directory with the MONAI_DATA_DIRECTORY environment variable.\n",
        "\n",
        "This allows you to save results and reuse downloads.\n",
        "\n",
        "If not specified a temporary directory will be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fc58c80",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "8fc58c80"
      },
      "outputs": [],
      "source": [
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
        "print(root_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a36b12f0",
      "metadata": {
        "id": "a36b12f0"
      },
      "source": [
        "## Set deterministic training for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad5a1948",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ad5a1948"
      },
      "outputs": [],
      "source": [
        "set_determinism(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b41e37b3",
      "metadata": {
        "id": "b41e37b3"
      },
      "source": [
        "## Setup MedNIST Dataset and training and validation dataloaders\n",
        "In this tutorial, we will train our models on the MedNIST dataset available on MONAI\n",
        "(https://docs.monai.io/en/stable/apps.html#monai.apps.MedNISTDataset). In order to train faster, we will select just\n",
        "one of the available classes (\"Hand\"), resulting in a training set with 7999 2D images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65e1c200",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "65e1c200"
      },
      "outputs": [],
      "source": [
        "############################################################\n",
        "#train_data = MedNISTDataset(root_dir=root_dir, section=\"training\", download=True, progress=False, seed=0)\n",
        "#train_datalist = [{\"image\": item[\"image\"]} for item in train_data.data if item[\"class_name\"] == \"Hand\"]\n",
        "\n",
        "CLASE_ELEGIDA = 7\n",
        "\n",
        "train_data = PathMNIST(split=\"train\", download=True, size=28)\n",
        "train_datalist = [\n",
        "    {\"image\": img}\n",
        "    for img, label in zip(train_data.imgs, train_data.labels)\n",
        "    if label[0] == CLASE_ELEGIDA\n",
        "][:8000]\n",
        "############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d503ec9",
      "metadata": {
        "id": "5d503ec9"
      },
      "source": [
        "Here we use transforms to augment the training dataset:\n",
        "\n",
        "1. `LoadImaged` loads the hands images from files.\n",
        "1. `EnsureChannelFirstd` ensures the original data to construct \"channel first\" shape.\n",
        "1. `ScaleIntensityRanged` extracts intensity range [0, 255] and scales to [0, 1].\n",
        "1. `RandAffined` efficiently performs rotate, scale, shear, translate, etc. together based on PyTorch affine transform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2f9bebd",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "e2f9bebd"
      },
      "outputs": [],
      "source": [
        "############################################################\n",
        "def rgb_to_gray_cv2(d):\n",
        "    img = d[\"image\"]\n",
        "    if img.ndim == 3 and img.shape[2] == 3:\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)  # H,W\n",
        "        d[\"image\"] = gray[:, :, None] # agrega dimensión de canal -> 1,H,W\n",
        "    return d\n",
        "############################################################\n",
        "\n",
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        ############################################################\n",
        "        #transforms.LoadImaged(keys=[\"image\"]),\n",
        "        #transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
        "        #transforms.Lambda(func=rgb_to_gray_cv2),\n",
        "        transforms.EnsureChannelFirstd(keys=[\"image\"], channel_dim=-1),\n",
        "        transforms.ResizeD(keys=[\"image\"], spatial_size=(64, 64)),\n",
        "        ############################################################\n",
        "\n",
        "        transforms.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
        "        transforms.RandAffined(\n",
        "            keys=[\"image\"],\n",
        "            rotate_range=[(-np.pi / 36, np.pi / 36), (-np.pi / 36, np.pi / 36)],\n",
        "            translate_range=[(-1, 1), (-1, 1)],\n",
        "            scale_range=[(-0.05, 0.05), (-0.05, 0.05)],\n",
        "            spatial_size=[64, 64],\n",
        "            padding_mode=\"zeros\",\n",
        "            prob=0.5,\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "############################################################\n",
        "#train_ds = CacheDataset(data=train_datalist, transform=train_transforms)\n",
        "#train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=4, persistent_workers=True)\n",
        "train_ds = Dataset(data=train_datalist, transform=train_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2, persistent_workers=True)\n",
        "############################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "938318c2",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "938318c2"
      },
      "outputs": [],
      "source": [
        "############################################################\n",
        "#val_data = MedNISTDataset(root_dir=root_dir, section=\"validation\", download=True, progress=False, seed=0)\n",
        "#val_datalist = [{\"image\": item[\"image\"]} for item in val_data.data if item[\"class_name\"] == \"Hand\"]\n",
        "\n",
        "val_data = PathMNIST(split=\"val\", download=True, size=28)\n",
        "\n",
        "val_datalist = [\n",
        "    {\"image\": img}\n",
        "    for img, label in zip(val_data.imgs, val_data.labels)\n",
        "    if label[0] == CLASE_ELEGIDA\n",
        "][:1000]\n",
        "############################################################\n",
        "\n",
        "val_transforms = transforms.Compose(\n",
        "    [\n",
        "        ############################################################\n",
        "        #transforms.LoadImaged(keys=[\"image\"]),\n",
        "        #transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
        "        #transforms.Lambda(func=rgb_to_gray_cv2),\n",
        "        transforms.EnsureChannelFirstd(keys=[\"image\"], channel_dim=-1),\n",
        "        transforms.ResizeD(keys=[\"image\"], spatial_size=(64, 64)),\n",
        "        ############################################################\n",
        "        transforms.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
        "    ]\n",
        ")\n",
        "############################################################\n",
        "#val_ds = CacheDataset(data=val_datalist, transform=val_transforms)\n",
        "#val_loader = DataLoader(val_ds, batch_size=128, shuffle=False, num_workers=4, persistent_workers=True)\n",
        "val_ds = Dataset(data=val_datalist, transform=val_transforms)\n",
        "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=2, persistent_workers=True)\n",
        "############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a56a4e42",
      "metadata": {
        "id": "a56a4e42"
      },
      "source": [
        "### Visualisation of the training images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b698f4f8",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "b698f4f8"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "check_data = first(train_loader)\n",
        "print(f\"batch shape: {check_data['image'].shape}\")\n",
        "image_visualisation = torch.cat(\n",
        "    [check_data[\"image\"][0, 0], check_data[\"image\"][1, 0], check_data[\"image\"][2, 0], check_data[\"image\"][3, 0]], dim=1\n",
        ")\n",
        "plt.figure(\"training images\", (12, 6))\n",
        "plt.imshow(image_visualisation, vmin=0, vmax=1, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "check_data = first(train_loader)\n",
        "img_tensor = check_data[\"image\"]\n",
        "print(f\"Batch shape: {img_tensor.shape}\")\n",
        "\n",
        "images_to_show = [img_tensor[i] for i in range(4)]\n",
        "\n",
        "\n",
        "if img_tensor.shape[1] == 3:\n",
        "    combined = torch.cat(images_to_show, dim=2)\n",
        "    image_visualisation = combined.permute(1, 2, 0).cpu().numpy()\n",
        "    cmap = None\n",
        "else:\n",
        "    combined = torch.cat(images_to_show, dim=2)\n",
        "    image_visualisation = combined.squeeze().cpu().numpy()\n",
        "    cmap = \"gray\"\n",
        "\n",
        "plt.figure(\"Visualización PathMNIST\", figsize=(12, 6))\n",
        "plt.imshow(image_visualisation, vmin=0, vmax=1, cmap=cmap)\n",
        "plt.axis(\"off\")\n",
        "plt.title(f\"Muestra de entrenamiento - Clase: {CLASE_ELEGIDA}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3090350",
      "metadata": {
        "id": "d3090350"
      },
      "source": [
        "### Define network, scheduler, optimizer, and inferer\n",
        "At this step, we instantiate the MONAI components to create a DDPM, the UNET, the noise scheduler, and the inferer used for training and sampling. We are using\n",
        "the original DDPM scheduler containing 1000 timesteps in its Markov chain, and a 2D UNET with attention mechanisms\n",
        "in the 2nd and 3rd levels, each with 1 attention head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c52e4f4",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "lines_to_next_cell": 0,
        "id": "2c52e4f4"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "model = DiffusionModelUNet(\n",
        "    spatial_dims=2,\n",
        "    ############################################################\n",
        "    #in_channels=1,\n",
        "    #out_channels=1,\n",
        "    #num_channels=(128, 256, 256),\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    channels=(128, 256, 256),\n",
        "    ############################################################\n",
        "    attention_levels=(False, True, True),\n",
        "    num_res_blocks=1,\n",
        "    num_head_channels=256,\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=2.5e-5)\n",
        "\n",
        "inferer = DiffusionInferer(scheduler)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a316067",
      "metadata": {
        "id": "5a316067"
      },
      "source": [
        "### Model training\n",
        "Here, we are training our model for 75 epochs (training time: ~50 minutes).\n",
        "\n",
        "If you would like to skip the training and use a pre-trained model instead, set `use_pretrained=True`. This model was trained using the code in `tutorials/generative/distributed_training/ddpm_training_ddp.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f697a13",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "lines_to_next_cell": 0,
        "id": "0f697a13"
      },
      "outputs": [],
      "source": [
        "use_pretrained = False\n",
        "\n",
        "EPOCAS_ELEGIDAS = 35\n",
        "\n",
        "if use_pretrained:\n",
        "    model = torch.hub.load(\"marksgraham/pretrained_generative_models:v0.2\", model=\"ddpm_2d\", verbose=True).to(device)\n",
        "else:\n",
        "    n_epochs = EPOCAS_ELEGIDAS\n",
        "    val_interval = 5\n",
        "    epoch_loss_list = []\n",
        "    val_epoch_loss_list = []\n",
        "\n",
        "    scaler = GradScaler()\n",
        "    total_start = time.time()\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=70)\n",
        "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "        for step, batch in progress_bar:\n",
        "            images = batch[\"image\"].to(device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            with autocast(enabled=True):\n",
        "                # Generate random noise\n",
        "                noise = torch.randn_like(images).to(device)\n",
        "\n",
        "                # Create timesteps\n",
        "                timesteps = torch.randint(\n",
        "                    0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
        "                ).long()\n",
        "\n",
        "                # Get model prediction\n",
        "                noise_pred = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
        "\n",
        "                loss = F.mse_loss(noise_pred.float(), noise.float())\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            progress_bar.set_postfix({\"loss\": epoch_loss / (step + 1)})\n",
        "        epoch_loss_list.append(epoch_loss / (step + 1))\n",
        "\n",
        "        if (epoch + 1) % val_interval == 0:\n",
        "            model.eval()\n",
        "            val_epoch_loss = 0\n",
        "            for step, batch in enumerate(val_loader):\n",
        "                images = batch[\"image\"].to(device)\n",
        "                with torch.no_grad():\n",
        "                    with autocast(enabled=True):\n",
        "                        noise = torch.randn_like(images).to(device)\n",
        "                        timesteps = torch.randint(\n",
        "                            0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
        "                        ).long()\n",
        "                        noise_pred = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
        "                        val_loss = F.mse_loss(noise_pred.float(), noise.float())\n",
        "\n",
        "                val_epoch_loss += val_loss.item()\n",
        "                progress_bar.set_postfix({\"val_loss\": val_epoch_loss / (step + 1)})\n",
        "            val_epoch_loss_list.append(val_epoch_loss / (step + 1))\n",
        "\n",
        "            # Sampling image during training\n",
        "            noise = torch.randn((1, 3, 64, 64))\n",
        "            noise = noise.to(device)\n",
        "            scheduler.set_timesteps(num_inference_steps=1000)\n",
        "            with autocast(enabled=True):\n",
        "                image = inferer.sample(input_noise=noise, diffusion_model=model, scheduler=scheduler)\n",
        "\n",
        "            plt.figure(figsize=(2, 2))\n",
        "            ############################################################\n",
        "            #plt.imshow(image[0, 0].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
        "            plt.imshow(image[0].permute(1, 2, 0).cpu().numpy())\n",
        "            ############################################################\n",
        "            plt.tight_layout()\n",
        "            plt.axis(\"off\")\n",
        "            plt.show()\n",
        "\n",
        "    total_time = time.time() - total_start\n",
        "    print(f\"train completed, total time: {total_time}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "057f0d69",
      "metadata": {
        "id": "057f0d69"
      },
      "source": [
        "### Learning curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cdcda81",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "2cdcda81"
      },
      "outputs": [],
      "source": [
        "if not use_pretrained:\n",
        "    plt.style.use(\"seaborn-v0_8\")\n",
        "    plt.title(\"Learning Curves\", fontsize=20)\n",
        "    plt.plot(np.linspace(1, n_epochs, n_epochs), epoch_loss_list, color=\"C0\", linewidth=2.0, label=\"Train\")\n",
        "    plt.plot(\n",
        "        np.linspace(val_interval, n_epochs, int(n_epochs / val_interval)),\n",
        "        val_epoch_loss_list,\n",
        "        color=\"C1\",\n",
        "        linewidth=2.0,\n",
        "        label=\"Validation\",\n",
        "    )\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.xlabel(\"Epochs\", fontsize=16)\n",
        "    plt.ylabel(\"Loss\", fontsize=16)\n",
        "    plt.legend(prop={\"size\": 14})\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d61cffe",
      "metadata": {
        "id": "2d61cffe"
      },
      "source": [
        "### Plotting sampling process along DDPM's Markov chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1427e5d4",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "1427e5d4"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "############################################################\n",
        "#noise = torch.randn((1, 1, 64, 64))\n",
        "noise = torch.randn((1, 3, 64, 64))\n",
        "############################################################\n",
        "noise = noise.to(device)\n",
        "scheduler.set_timesteps(num_inference_steps=1000)\n",
        "with autocast(enabled=True):\n",
        "    image, intermediates = inferer.sample(\n",
        "        input_noise=noise, diffusion_model=model, scheduler=scheduler, save_intermediates=True, intermediate_steps=100\n",
        "    )\n",
        "\n",
        "chain = torch.cat(intermediates, dim=-1)\n",
        "\n",
        "plt.style.use(\"default\")\n",
        "############################################################\n",
        "#plt.imshow(chain[0, 0].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
        "plt.imshow(chain[0].permute(1, 2, 0).cpu().numpy())\n",
        "############################################################\n",
        "plt.tight_layout()\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c45cead",
      "metadata": {
        "id": "1c45cead"
      },
      "source": [
        "### Cleanup data directory\n",
        "\n",
        "Remove directory if a temporary was used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bab2d719",
      "metadata": {
        "id": "bab2d719"
      },
      "outputs": [],
      "source": [
        "if directory is None:\n",
        "    shutil.rmtree(root_dir)"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "formats": "ipynb,py:percent"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}